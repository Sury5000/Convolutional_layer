{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEe41nlxu_yc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.datasets import load_sample_images\n",
        "\n",
        "\n",
        "sample_images = np.stack(load_sample_images()[\"images\"])\n",
        "sample_images = torch.tensor(sample_images, dtype = torch.float32) / 255"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_images.shape\n",
        "# height 427, width 630 and 3 channels red,green,blue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WmEzpd2wFHH",
        "outputId": "cf18bdde-cd08-44c8-aab8-8333df2709c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 427, 640, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_images_permuted = sample_images.permute(0,3,1,2)\n",
        "sample_images_permuted.shape\n",
        "\n",
        "# Pytorch expects channel to be before height and width so we change the using permute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORQ67GgcwKOf",
        "outputId": "e47647ef-0d36-42d6-83b6-97396d44b5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 427, 640])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "\n",
        "cropped_images = T.CenterCrop((70, 120))(sample_images_permuted)\n",
        "cropped_images.shape\n",
        "\n",
        "# we used center crop function to force the incoming dimensions to converted to 70 X 120 this make sure any image come within the crop function converted to this dimension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPewqNRHwm70",
        "outputId": "79cacf40-e771-427a-b840-b2e6a14e7f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 70, 120])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now using 2d convolutional layer to process the cropped image\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "conv_layer = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 7)\n",
        "fmaps = conv_layer(cropped_images)"
      ],
      "metadata": {
        "id": "k96fv-e_xPSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fmaps.shape\n",
        "\n",
        "# we used conv2d to convert the image pixels to feature maps by giving pixel size 7x7 in the receptive field so it can take 7x7 pizxels as batches to identify images\n",
        "# Then we convert the channel from 3 to 32 feature maps as various features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP77xL9Vx6l_",
        "outputId": "dd8d04cc-cd3a-446c-8378-5c0235c47fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 32, 64, 114])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer2 = nn.Conv2d(in_channels = 3, out_channels=32, kernel_size = 7, padding = \"same\")\n",
        "\n",
        "fmaps2 = conv_layer2(cropped_images)\n",
        "fmaps2.shape\n",
        "\n",
        "# we can use padding \"same\" to get every pixel to learned from convolutional layer"
      ],
      "metadata": {
        "id": "eXXkm4xYzZ8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90da71d7-35f3-4ea2-a876-a6b489112d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 32, 70, 120])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer2.weight.shape\n",
        "# out_channel, in_channel, kernel_height, kernel_width"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8pRckUduoMc",
        "outputId": "538e9123-4dd0-48bc-b556-55278565409a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 7, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer2.bias.shape\n",
        "\n",
        "# every feature map in a convalutional layer shares the same parameter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NPEkWR3xQYS",
        "outputId": "b791fe29-79f0-48ff-dd62-f3ed97fdc614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxpool = nn.MaxPool2d(kernel_size = 2)\n",
        "\n",
        "# Pooling layers reduces the size of an image from the first convolutional layer by retaining only the important pixels\n",
        "# here we used maxpool which only get maximum intensed pixel of the particular kernel"
      ],
      "metadata": {
        "id": "GDI-31xtxjrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class DepthPool(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, kernel_size, stride = None, padding = 0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = stride if stride is not None else kernel_size\n",
        "    self.padding = padding\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    batch, channels, height, width = inputs.shape\n",
        "\n",
        "    z = inputs.view(batch, channels, height * width)\n",
        "    z = z.permute(0,2,1)\n",
        "    z = F.max_pool1d(z, kernel_size = self.kernel_size, stride = self.stride, padding = self.padding)\n",
        "    z = z.permute(0,2,1)\n",
        "\n",
        "    return z.view(batch, -1, height, width)\n",
        "\n",
        "# Th depthpool function will take a input and shift the shape by interchange the height * width with channel so we can do max pooling on channels wise\n",
        "# finally change it back to normal size by permuting again with the changed size\n"
      ],
      "metadata": {
        "id": "Re4SLbuh4HrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_avg_pool = nn.AdaptiveAvgPool2d(output_size = 1)\n",
        "output_pool = global_avg_pool(cropped_images)\n",
        "output_pool\n",
        "\n",
        "# Global average pool reduces informations from all feature maps into single output but it reduces most of the informations\n",
        "# we take overall from an image shape in the final layer to detect the image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZJr9eVm7avM",
        "outputId": "fa0cca7d-66c7-403f-e6a3-c35251c169c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.6434]],\n",
              "\n",
              "         [[0.5972]],\n",
              "\n",
              "         [[0.5825]]],\n",
              "\n",
              "\n",
              "        [[[0.7631]],\n",
              "\n",
              "         [[0.2601]],\n",
              "\n",
              "         [[0.1085]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5B9Slzu-L21"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}